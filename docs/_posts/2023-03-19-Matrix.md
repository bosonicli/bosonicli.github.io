---
title: Matrix
date: 2023-03-19
author: bosonicli
tags:
-   Ether
---

[toc]

# Diag

$$
\begin{aligned}
    if & \quad P^{\dagger}_{ n \times n } = P_{ n \times n }  \\
    esits & \quad Q_{ n \times n } \quad and \quad \Lambda_{ n \times n } \\
    where & \quad Q^{\dagger} Q = I_{n}   \\
    and & \quad \Lambda = diag_{n}(\lambda_{i})    \\
    s.t. & \quad P = Q \Lambda Q^{\dagger}  \\
    PQ &= Q \Lambda \\
    P Q_{i} &= Q_{i} \lambda_{i}
\end{aligned}
$$

Columns of \\(Q\\) are EigenVectors of Hermitian Matrix \\(P\\) with EigenValues \\(\Lambda\\)

# Trace

$$
\begin{aligned}
    Tr(A_{ m \times m}) & \equiv \Sigma_{i=1}^{m} A_{i,i}   \\
    Tr(( A_{ m \times n } B_{ n \times m} )_{ m \times m}) &= \Sigma_{i=1}^{m} ( A_{ m \times n } B_{ n \times m} )_{i,i}   \\
    &= \Sigma_{i=1}^{m} ( \Sigma_{j=1}^{n} A_{i,j} B_{j,i} )   \\
    &= \Sigma_{j=1}^{n} ( \Sigma_{i=1}^{m} B_{j,i} A_{i,j} )    \\
    &= Tr(( B_{ n \times m} A_{ m \times n } )_{ n \times n})
\end{aligned}
$$

# Det / Inv

$$
\begin{aligned}
    A^{-1} &= \frac{1}{det(A)} cof(A)   \\
    cof(A)_{ij} & \equiv (-)^{i+j} det(A_{ji})  \\
    \begin{pmatrix}
        a \quad b \\
        c \quad d
    \end{pmatrix}
    ^{-1} &=
    \frac{1}{ad-bc}
    \begin{pmatrix}
        d \quad & -c  \\
        -b \quad & a
    \end{pmatrix}
\end{aligned}
$$

$$
\begin{aligned}
    Tr( Q \Lambda Q^{\dagger} ) &= Tr( Q^{\dagger} Q \Lambda )  \\
    &= Tr(\Lambda)
\end{aligned}
$$

# Projection

If \\( \exist \\) hermitian matrix \\(P_{ n \times n }\\) satisfying

$$
\begin{aligned}
    P^{\dagger}_{ n \times n } &= P_{ n \times n }    \\
    P^{2}_{ n \times n } &= P_{ n \times n }
\end{aligned}
$$

Then by operating Matrix \\(P_{ n \times n }\\) on Vector \\(X_{n}\\) we got

$$
\begin{aligned}
    Y &= P X \\
    Y^{\dagger} Y &= X^{\dagger} P^{\dagger} P X    \\
    &= X^{\dagger} P P X    \\
    &= X^{\dagger} P X  \\
    &= X^{\dagger} Y    \\
    (X-Y)^{\dagger} Y &= 0  \\
    X-Y & \perp Y
\end{aligned}
$$

So \\(Y=PX\\) is a Projection of \\(X\\)

Since \\(P\\) is hermitian, it can be diagonalize

$$
\begin{aligned}
    \lambda_{i}(P) &= 0, 1  \\
    P &= Q \Lambda_{0,1} Q^{\dagger}    \\
    &=
    \begin{pmatrix}
        Q_{1} \quad Q_{2}
    \end{pmatrix}
    \begin{pmatrix}
        I_{r} \quad 0   \\
        0 \quad 0
    \end{pmatrix}
    \begin{pmatrix}
        Q_{1}^{\dagger} \\
        Q_{2}^{\dagger}
    \end{pmatrix}
    \\
    &= Q_{1} I_{r} Q_{1}^{\dagger}  \\
    &= Q_{1} (Q_{1}^{\dagger}Q_{1})^{-1} Q_{1}^{\dagger}    \\
    Tr(P) &= Tr(I_{r})   \\
    &= r(P)
\end{aligned}
$$

where \\(r=r(P)\\) is non-0 eigenvalue of \\(P\\), aka Rank of \\\(P\\)

If we consider

$$
\begin{aligned}
    P_{ n \times n } &= Q ( Q^{\dagger} Q )_{ r \times r }^{-1} Q^{\dagger} \\
    for & \quad Q_{ n \times r } \quad ( r \leq n ) \\
    P^{\dagger} &= P    \\
    P^{2} &= P  \\
    Tr(P) &= Tr( (Q^{\dagger}Q)_{ r \times r } (Q^{\dagger}Q)_{ r \times r }^{-1} ) \\
    &= Tr(I_{r})    \\
    &= r
\end{aligned}
$$

So

$$
\begin{aligned}
    P_{ n \times n } \quad is \quad rank(r) & \quad Projection \\
    & \Leftrightarrow   \\
    \exist \quad Q_{ n \times r } \quad ( r \leq n) \quad & s.t. \quad P = Q ( Q^{\dagger} Q )^{-1} Q^{\dagger}
\end{aligned}
$$

Note that

$$
\begin{aligned}
    P Q &= Q
\end{aligned}
$$

which means \\(Q\\) is invariant under Projection \\(P\\) from \\(n\\)-dim space to \\(r\\)-dim subspace.

If \\(Q_{ n \times r }( r \leq n )\\) is column-full-rank (rank is \\(r\\) ), then the Projection Subspace is expanded by \\(Q\\)

# SVD

$$
\begin{aligned}
    X_{ m \times n } &= U_{ m \times m } D_{ m \times n } V_{ n \times n }^{\dagger}    \\
    where & \quad U^{\dagger} U = I_{m} \\
    and & \quad V^{\dagger} V = I_{n}
\end{aligned}
$$

Then

$$
\begin{aligned}
    ( X X^{\dagger} )_{ m \times m } &= U \Lambda_{m} U^{\dagger}   \\
    \Lambda_{m} &= D_{ m \times n } D_{ n \times m }^{\dagger}  \\
    ( X X^{\dagger} ) U &= U \Lambda_{m}   \\
    ( X^{\dagger} X )_{ n \times n } &= V \Lambda_{n} V^{\dagger}   \\
    \Lambda_{n} &= D_{ n \times m }^{\dagger} D_{ m \times n }  \\
    ( X^{\dagger} X ) V &= V \Lambda_{n}    \\
    ( X X^{\dagger} ) ( X V ) &= ( X V ) \Lambda_{m}    \\
    ( X^{\dagger} X ) ( X^{\dagger} U ) &= ( X^{\dagger} U ) \Lambda_{n}
\end{aligned}
$$

Apparently \\( U / V \\) are EigenSpace of \\( X X^{\dagger} / X^{\dagger} X \\) respectively and can transform via \\( X / X^{\dagger} \\).

If for Transform Matrix

$$
\begin{aligned}
    A_{ m \times n } &= U_{ m \times m } D_{ m \times n } V_{ n \times n }^{\dagger}
\end{aligned}
$$

we define an Inverse Matrix

$$
\begin{aligned}
    A_{ n \times m }^{-1} &= V_{ n \times n } D_{ n \times m }^{-1} U_{ m \times m }^{\dagger}
\end{aligned}
$$

then we have

$$
\begin{aligned}
    A^{-1} A &= V V^{\dagger} = I_{n}   \\
    A A^{-1} &= U U^{\dagger} = I_{m}
\end{aligned}
$$

In Linear Regression, if we have a too large Covariance Matrix

$$
\begin{aligned}
    \mathbb{D}(X_{r}) &= X_{ r \times n }^{\dagger} X_{ n \times r }    \\
    &= V_{ r \times r } \Lambda_{r} V_{ r \times r }^{\dagger}
\end{aligned}
$$

Then we can Reduce the Data Dimension \\(r\\) by applying a Linear Transform

$$
\begin{aligned}
    Y_{ n \times s } &= X_{ n \times r } P_{ r \times s } \quad ( s \leq r )  \\
    \mathbb{D}(Y_{s}) &= Y^{\dagger} Y  \\
    &= P^{\dagger} X^{\dagger} X P  \\
    &= ( P^{\dagger} V ) \Lambda_{r} ( P^{\dagger} V )^{\dagger}
\end{aligned}
$$

If

$$
\begin{aligned}
    If & \quad P_{ r \times s } =
    \begin{pmatrix}
        V_{1}
        \quad \dots
        \quad V_{s}
    \end{pmatrix}   \\
    Then & \quad P_{ s \times r }^{\dagger} V_{ r \times r } =
    \begin{pmatrix}
        I_{s} \quad 0
    \end{pmatrix}_{ s \times r }    \\
    We \quad get & \quad \mathbb{D}(Y_{s}) = \Lambda_{s}
\end{aligned}
$$

Then we Reduced the Dimension of Covariant Matrix while preserving its most important EigenValues
